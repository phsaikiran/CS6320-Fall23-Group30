{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6388026e-c143-44f2-8e90-171f70b5408c",
   "metadata": {},
   "source": [
    "# Load train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I booked two rooms four months in advance at the Talbott . We were placed on the top floor next to the elevators , which are used all night long . When speaking to the front desk , I was told that they were simply honoring my request for an upper floor , which I had requested for a better view . I am looking at a brick wall , and getting no sleep . He also told me that they had received complaints before from guests on the 16th floor , and were aware of the noise problem . Why then did they place us on this floor when the hotel is not totally booked ? A request for an upper floor does not constitute placing someone on the TOP floor and using that request to justify this . If you decide to stay here , request a room on a lower floor and away from the elevator ! I spoke at length when booking my two rooms about my preferences . This is simply poor treatment of a guest whom they believed would not complain .\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines = []\n",
    "with open('A1_DATASET/train.txt', 'r') as f:\n",
    "    train_lines = f.readlines()\n",
    "train_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f69fb3-cce0-427d-9313-5baada64c135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I stayed for four nights while attending a conference . The hotel is in a great spot - easy walk to Michigan Ave shopping or Rush St. , but just off the busy streets . The room I had was spacious , and very well-appointed . The staff was friendly , and the fitness center , while not huge , was well-equipped and clean . I 've stayed at a number of hotels in Chicago , and this one is my favorite . Internet was n't free , but at $ 10 for 24 hours is cheaper than most business hotels , and it worked very well .\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_lines = []\n",
    "with open('A1_DATASET/val.txt', 'r') as f:\n",
    "    val_lines = f.readlines()\n",
    "val_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eae6af-d7d2-4ce9-847f-a3b9dfbf6692",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b07b656-d6da-4ea8-93e8-feb043779813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saiki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecfec4fc-5269-416b-86cd-8714e5cfbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess(line):\n",
    "    line = line.lower()\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    #tokens = line.split(\" \")\n",
    "    #tokens = list(filter(lambda x: len(x)!=1 or (len(x)==1 and x not in string.punctuation), tokens))\n",
    "    #tokens = list(filter(lambda x: len(x) != 0, tokens))\n",
    "    tokens = [\"<START>\"] + tokens + [\"<END>\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00686116-6d01-4e27-ac10-a971fbcc2000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'pen',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'book',\n",
       " ',',\n",
       " 'and',\n",
       " '<END>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"I have a pen .   I don't have a book, and    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02f25b-3c00-4e95-9311-65192358b872",
   "metadata": {},
   "source": [
    "# Unsmoothed Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33973413-deaf-4f54-9619-e3459b6a502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "def get_vocab(lines):\n",
    "    vocab = []\n",
    "    unigrams = {}\n",
    "    bigrams = {}\n",
    "    for line in lines:\n",
    "        preprocess_line = preprocess(line)\n",
    "        for token in preprocess_line:\n",
    "            if token in unigrams.keys():\n",
    "                unigrams[token] += 1\n",
    "            else:\n",
    "                unigrams[token] = 1\n",
    "\n",
    "        for token, next_token in list(pairwise(preprocess_line)):\n",
    "            if (token, next_token) in bigrams.keys():\n",
    "                bigrams[(token, next_token)] += 1\n",
    "            else:\n",
    "                bigrams[(token, next_token)] = 1\n",
    "\n",
    "        vocab.extend(preprocess_line)\n",
    "\n",
    "    vocab = set(vocab)\n",
    "    return vocab, unigrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74142b81-f2e3-4497-b322-cfa97f04a1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6332, 90788, 90276)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab, unigram_counts, bigram_counts = get_vocab(train_lines)\n",
    "vocab_len = len(vocab)\n",
    "total_unigrams = np.sum(list(unigram_counts.values()))\n",
    "total_bigrams = np.sum(list(bigram_counts.values()))\n",
    "\n",
    "vocab_len, total_unigrams, total_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3a968b-8fd9-4f37-b8a9-daa5177d2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probability = {}\n",
    "for u in unigram_counts.keys():\n",
    "    unigram_probability[u] = unigram_counts[u]/total_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c3305c-9e6b-47dd-83aa-5182fcaa768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probability = {}\n",
    "for b in bigram_counts.keys():\n",
    "    bigram_probability[b] = bigram_counts[b]/unigram_counts[b[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa446ba-94cc-4089-a198-d8263ac4e361",
   "metadata": {},
   "source": [
    "# Unknown word handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74aef955-df93-411a-857c-8be74e4c350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3116"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words = []\n",
    "for u in unigram_counts.keys():\n",
    "    if unigram_counts[u] < 2:\n",
    "        unknown_words += [u]\n",
    "\n",
    "vocab = vocab - set(unknown_words)\n",
    "vocab.add(\"<UNK>\")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761df26d-3e3a-4bbd-817b-9666309702ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "def get_vocab_unknown(lines):\n",
    "    unigrams = {}\n",
    "    bigrams = {}\n",
    "    for line in lines:\n",
    "        preprocess_line = preprocess(line)\n",
    "        for token in preprocess_line:\n",
    "            if token not in vocab:\n",
    "                token = '<UNK>'\n",
    "            \n",
    "            if token in unigrams.keys():\n",
    "                unigrams[token] += 1\n",
    "            else:\n",
    "                unigrams[token] = 1\n",
    "\n",
    "        for token, next_token in list(pairwise(preprocess_line)):\n",
    "            if token not in vocab:\n",
    "                token = '<UNK>'\n",
    "            if next_token not in vocab:\n",
    "                next_token = '<UNK>'\n",
    "                \n",
    "            if (token, next_token) in bigrams.keys():\n",
    "                bigrams[(token, next_token)] += 1\n",
    "            else:\n",
    "                bigrams[(token, next_token)] = 1\n",
    "\n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb3dbf5-49d1-4bac-9c83-5768d67e7463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3116, 90788, 90276)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_counts_unk, bigram_counts_unk = get_vocab_unknown(train_lines)\n",
    "vocab_len = len(vocab)\n",
    "total_unigrams = np.sum(list(unigram_counts.values()))\n",
    "total_bigrams = np.sum(list(bigram_counts.values()))\n",
    "\n",
    "vocab_len, total_unigrams, total_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a07e5164-5415-4343-a025-6a0b3409e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probability = {}\n",
    "for u in unigram_counts.keys():\n",
    "    unigram_probability[u] = unigram_counts[u]/total_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81766717-2e7d-4fc0-be7f-5524601443cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probability = {}\n",
    "for b in bigram_counts.keys():\n",
    "    bigram_probability[b] = bigram_counts[b]/unigram_counts[b[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0a7ae-4792-4db8-a140-0cfc268c7326",
   "metadata": {},
   "source": [
    "# Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4b7439-2f71-412a-9a22-d624f1b3d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(uc, bc):\n",
    "    laplace_unigram = {}\n",
    "    for uc_item in uc.keys():\n",
    "        laplace_unigram[uc_item] = (uc[uc_item] + 1) / (total_unigrams + vocab_len)\n",
    "\n",
    "    laplace_bigram = {}\n",
    "    for bc_item in bc.keys():\n",
    "        laplace_bigram[bc_item] = (bc[bc_item] + 1) / (uc[bc_item[0]] + vocab_len)\n",
    "\n",
    "    return laplace_unigram, laplace_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece00857-492f-4972-b76c-478cb06c8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_unigram, laplace_bigram = laplace_smoothing(unigram_counts_unk, bigram_counts_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09165f-02d1-434e-9b15-e61d9a02717b",
   "metadata": {},
   "source": [
    "# k smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9638560-b0bd-4ca0-b92e-9162725f7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_smoothing(uc, bc, k):\n",
    "    k_unigram = {}\n",
    "    for uc_item in uc.keys():\n",
    "        k_unigram[uc_item] = (uc[uc_item] + k) / (total_unigrams + k*vocab_len)\n",
    "\n",
    "    k_bigram = {}\n",
    "    for bc_item in bc.keys():\n",
    "        k_bigram[bc_item] = (bc[bc_item] + k) / (uc[bc_item[0]] + k*vocab_len)\n",
    "\n",
    "    return k_unigram, k_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63eedcd-28a5-49af-a35f-2d5746baa465",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bc1ca1a-dcdd-4aa3-8997-eb5395cd898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_perplexity_unigram(lines, unigram_model):\n",
    "    perplexity = 0\n",
    "    N = 0\n",
    "    for line in lines:\n",
    "        preprocess_line = preprocess(line)\n",
    "        N += len(preprocess_line)\n",
    "        for token in preprocess_line:\n",
    "            if token in vocab:\n",
    "                perplexity += np.log(unigram_model.get(token))\n",
    "            else:\n",
    "                perplexity += np.log(unigram_model.get('<UNK>'))\n",
    "    return np.exp(-perplexity/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8314682-9136-4ea5-b82a-2ac17c6e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_perplexity_bigram(lines, bigram_model):\n",
    "    perplexity = 0\n",
    "    N = 0\n",
    "    for line in lines:\n",
    "        preprocess_line = preprocess(line)\n",
    "        N += len(preprocess_line)\n",
    "        for token, next_token in list(pairwise(preprocess_line)):\n",
    "            if token not in vocab:\n",
    "                token = '<UNK>'\n",
    "            if next_token not in vocab:\n",
    "                next_token = '<UNK>'\n",
    "    \n",
    "            perplexity += np.log(bigram_model.get((token, next_token), bigram_model.get(('<UNK>', '<UNK>'))))\n",
    "    \n",
    "    return np.exp(-perplexity/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66439593-5e14-444a-b15d-5df87702ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace unigram perplexity 294.5954530643091\n"
     ]
    }
   ],
   "source": [
    "up = get_val_perplexity_unigram(val_lines, laplace_unigram)\n",
    "print(\"Laplace unigram perplexity\", up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "923c6d1a-caa4-4b08-bb28-913c72370849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace bigram perplexity 150.82259673358692\n"
     ]
    }
   ],
   "source": [
    "bp = get_val_perplexity_bigram(val_lines, laplace_bigram)\n",
    "print(\"Laplace bigram perplexity\", bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "842bdc80-4c79-4510-b7a1-a957566d5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 0.01 unigram perplexity 292.3278883693682 bigram perplexity 31.92382054529934\n",
      "k 0.05 unigram perplexity 292.39553079628536 bigram perplexity 42.987511909458235\n",
      "k 0.1 unigram perplexity 292.48360966221304 bigram perplexity 52.560968636835966\n",
      "k 0.5 unigram perplexity 293.31228896255243 bigram perplexity 103.14391821707882\n",
      "k 0.75 unigram perplexity 293.92444817510864 bigram perplexity 128.07330094201447\n",
      "k 1 unigram perplexity 294.5954530643091 bigram perplexity 150.82259673358692\n",
      "k 1.5 unigram perplexity 296.08148913983433 bigram perplexity 192.09649783088886\n",
      "k 2 unigram perplexity 297.7198638726367 bigram perplexity 229.44216987257417\n",
      "k 5 unigram perplexity 309.3503045468004 bigram perplexity 408.2946572607758\n",
      "k 10 unigram perplexity 331.33496184513206 bigram perplexity 620.9639923061641\n"
     ]
    }
   ],
   "source": [
    "k_values = [0.01, 0.05, 0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10]\n",
    "for k in k_values:\n",
    "    k_unigram, k_bigram = k_smoothing(unigram_counts_unk, bigram_counts_unk, k)\n",
    "\n",
    "    up = get_val_perplexity_unigram(val_lines, k_unigram)\n",
    "    bp = get_val_perplexity_bigram(val_lines, k_bigram)\n",
    "\n",
    "    print(\"k\", k, \"unigram perplexity\", up, \"bigram perplexity\", bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238ec50-ce88-4b85-af59-8d40c9503091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
